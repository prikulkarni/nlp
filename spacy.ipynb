{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER, POS, lemmatize, remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Emma Woodhouse': 'PERSON', 'nearly twenty-one years': 'DATE', 'two': 'CARDINAL'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "sample_text = gutenberg.raw(\"sample_text.txt\")\n",
    "sample_text = sample_text.replace('\\n',' ')\n",
    "doc = nlp(str(sample_text))\n",
    "#print([(x.text, x.label_) for x in doc.ents])\n",
    "#displacy.render(doc, jupyter=True, style='ent')\n",
    "#print([(x.orth_, x.pos_, x.lemma_) for x in [y for y in doc if not y.is_stop and y.pos_ != 'PUNCT']])\n",
    "print(dict([(str(x), x.label_) for x in doc.ents]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
      "{'ner': 272.6808154482794}\n",
      "{'ner': 184.13004564504305}\n",
      "{'ner': 190.98931958647418}\n",
      "{'ner': 196.59636726640161}\n",
      "{'ner': 185.95594599906195}\n",
      "{'ner': 174.24127839228706}\n",
      "{'ner': 206.06090385467607}\n",
      "{'ner': 206.46684956641056}\n",
      "{'ner': 199.37481365501344}\n",
      "{'ner': 197.3634408730328}\n",
      "{'ner': 205.45417466586298}\n",
      "{'ner': 184.44423763495269}\n",
      "{'ner': 181.77947196863434}\n",
      "{'ner': 201.5698425594516}\n",
      "{'ner': 198.50400538809714}\n",
      "{'ner': 190.18621065631066}\n",
      "{'ner': 195.0055002172331}\n",
      "{'ner': 181.62375770402417}\n",
      "{'ner': 184.79830548918437}\n",
      "{'ner': 186.3375208502154}\n",
      "Entities in 'EU sanctions ineffective?\n",
      "'\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import plac\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "def offseter(lbl, doc, matchitem):\n",
    "    o_one = len(str(doc[0:matchitem[1]]))\n",
    "    subdoc = doc[matchitem[1]:matchitem[2]]\n",
    "    o_two = o_one + len(str(subdoc))\n",
    "    return (o_one, o_two, lbl)\n",
    "\n",
    "#nlp = spacy.blank('en')\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner)\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')\n",
    "\n",
    "#ner.add_label(label)\n",
    "\n",
    "label = 'CHANCEL'\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "for i in ['Angela Merkel', 'Angela', 'Merkel',]:\n",
    "    matcher.add(label, None, nlp(i))\n",
    "    \n",
    "label1 = 'PRESID'\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "for i in ['Vladimir Putin', 'Vladimir', 'Putin',]:\n",
    "    matcher.add(label1, None, nlp(i))\n",
    "\n",
    "    \n",
    "ner.add_label(label)\n",
    "ner.add_label(label1)\n",
    "\n",
    "res = []\n",
    "to_train_ents = []\n",
    "with open('angela_merkel.txt') as am:\n",
    "    line = True\n",
    "    while line:\n",
    "        line = am.readline()\n",
    "        mnlp_line = nlp(line)\n",
    "        matches = matcher(mnlp_line)\n",
    "        res = [offseter(label, mnlp_line, x)\n",
    "               for x\n",
    "               in matches]\n",
    "        to_train_ents.append((line, dict(entities=res)))\n",
    "        res1 = [offseter(label1, mnlp_line, x)\n",
    "               for x\n",
    "               in matches]\n",
    "        to_train_ents.append((line, dict(entities=res1)))\n",
    "\n",
    "@plac.annotations(\n",
    "    new_model_name=(\"New model name for model meta.\", \"option\", \"nm\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path))\n",
    "def train(new_model_name='angela', output_dir=None):\n",
    "\n",
    "    optimizer = nlp.begin_training()\n",
    "    \n",
    "    other_pipes = [pipe\n",
    "                   for pipe\n",
    "                   in nlp.pipe_names\n",
    "                   if pipe != 'ner']\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(20):\n",
    "            losses = {}\n",
    "            random.shuffle(to_train_ents)\n",
    "            for item in to_train_ents:\n",
    "                nlp.update([item[0]],\n",
    "                           [item[1]],\n",
    "                           sgd=optimizer,\n",
    "                           drop=0.35,\n",
    "                           losses=losses)\n",
    "            print(losses)\n",
    "\n",
    "    if output_dir is None:\n",
    "        output_dir = \"./angela\"\n",
    "\n",
    "\n",
    "    noutput_dir = Path(output_dir)\n",
    "    if not noutput_dir.exists():\n",
    "        noutput_dir.mkdir()\n",
    "        \n",
    "    nlp.meta['name'] = new_model_name\n",
    "    nlp.to_disk(output_dir)\n",
    "        \n",
    "    random.shuffle(to_train_ents)\n",
    "\n",
    "    test_text = to_train_ents[1][0]\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "train()\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#plac.call(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy's training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
      "{'ner': 14.903143644332888}\n",
      "{'ner': 10.633922471025244}\n",
      "{'ner': 9.301600298995215}\n",
      "{'ner': 10.423443384062693}\n",
      "{'ner': 14.36390769481659}\n",
      "{'ner': 12.244401382727439}\n",
      "{'ner': 8.81959519952943}\n",
      "{'ner': 12.92722288224082}\n",
      "{'ner': 12.006990670118773}\n",
      "{'ner': 7.36171617410659}\n",
      "{'ner': 4.611324977855412}\n",
      "{'ner': 10.10395162478596}\n",
      "{'ner': 5.712842330405726}\n",
      "{'ner': 4.599513058737772}\n",
      "{'ner': 4.366744292342068}\n",
      "{'ner': 7.081644639928038}\n",
      "{'ner': 4.705553461870732}\n",
      "{'ner': 2.64676956914952}\n",
      "{'ner': 0.0035026795699626178}\n",
      "{'ner': 2.016619148856186}\n",
      "{'ner': 5.928474605568221}\n",
      "{'ner': 2.020657978787887}\n",
      "{'ner': 6.8011948058400296e-06}\n",
      "{'ner': 1.1616648474021694e-05}\n",
      "{'ner': 1.7275064458807496}\n",
      "{'ner': 0.6595819481465341}\n",
      "{'ner': 6.090223747378934e-05}\n",
      "{'ner': 0.20586453416152736}\n",
      "{'ner': 1.152571531166199e-07}\n",
      "{'ner': 6.398419354428285e-09}\n",
      "{'ner': 1.9986677276934854}\n",
      "{'ner': 2.6686856842062624e-08}\n",
      "{'ner': 6.361919774896291e-09}\n",
      "{'ner': 0.0004923625617193417}\n",
      "{'ner': 5.075837347179976e-09}\n",
      "{'ner': 5.826432596609951e-13}\n",
      "{'ner': 9.123179575113927e-14}\n",
      "{'ner': 0.43063364557385847}\n",
      "{'ner': 2.394125203864e-06}\n",
      "{'ner': 1.1134331411017763e-17}\n",
      "{'ner': 0.5146418213844389}\n",
      "{'ner': 0.00031188101276582504}\n",
      "{'ner': 3.130116309147113e-10}\n",
      "{'ner': 0.011445276439202469}\n",
      "{'ner': 1.7329758007400882e-10}\n",
      "{'ner': 2.2216762649904872e-10}\n",
      "{'ner': 2.47641450282294e-17}\n",
      "{'ner': 1.8808973773519144e-14}\n",
      "{'ner': 2.5485103798257047e-10}\n",
      "{'ner': 5.330704485339615e-19}\n",
      "{'ner': 2.9319782768131964e-16}\n",
      "{'ner': 8.506947751168747e-12}\n",
      "{'ner': 1.246984724731135e-13}\n",
      "{'ner': 3.679983748344385e-11}\n",
      "{'ner': 5.412169657752808e-11}\n",
      "{'ner': 1.575943482745183e-11}\n",
      "{'ner': 0.0022953815485516147}\n",
      "{'ner': 8.051369500356118e-13}\n",
      "{'ner': 6.837550421626761e-07}\n",
      "{'ner': 7.675212883257071e-14}\n",
      "{'ner': 1.0380308488394015e-09}\n",
      "{'ner': 4.8190297283082204e-14}\n",
      "{'ner': 0.023646888046641834}\n",
      "{'ner': 8.438725924194238e-15}\n",
      "{'ner': 4.364977564176702e-12}\n",
      "{'ner': 7.1883123745931995e-09}\n",
      "{'ner': 1.3630952100343365e-20}\n",
      "{'ner': 8.727262039115204e-13}\n",
      "{'ner': 1.586097195805552e-07}\n",
      "{'ner': 1.5992909932082227e-17}\n",
      "{'ner': 2.6543059610430435e-25}\n",
      "{'ner': 1.628089454005178e-17}\n",
      "{'ner': 1.8765668637056274e-21}\n",
      "{'ner': 0.00014120322980994593}\n",
      "{'ner': 4.576524803025937e-09}\n",
      "{'ner': 1.6532581136241985e-12}\n",
      "{'ner': 1.0230217556100329e-11}\n",
      "{'ner': 1.8918199808751117e-14}\n",
      "{'ner': 6.92571569538286e-10}\n",
      "{'ner': 0.00018609673134051263}\n",
      "{'ner': 2.767844192025794e-16}\n",
      "{'ner': 4.14506011036018e-20}\n",
      "{'ner': 2.927124328886484e-05}\n",
      "{'ner': 3.707502636254211e-18}\n",
      "{'ner': 3.989755963256658e-19}\n",
      "{'ner': 1.498415023814681e-21}\n",
      "{'ner': 7.55442619462804e-15}\n",
      "{'ner': 3.1399441976129892e-21}\n",
      "{'ner': 3.36427945132736e-24}\n",
      "{'ner': 1.8015821723338377e-08}\n",
      "{'ner': 1.3979541171540879e-22}\n",
      "{'ner': 2.336642181089334e-24}\n",
      "{'ner': 7.052928662107131e-13}\n",
      "{'ner': 7.203071353930343e-22}\n",
      "{'ner': 3.9733629720317035e-09}\n",
      "{'ner': 2.2101749388669903e-21}\n",
      "{'ner': 2.7626862948361567e-14}\n",
      "{'ner': 1.150574508938014e-24}\n",
      "{'ner': 6.19361780089309e-19}\n",
      "{'ner': 6.04034525983471e-15}\n",
      "Entities [('Shakira', 'PERSON')]\n",
      "Tokens [('Who', '', 2), ('is', '', 2), ('Shakira', 'PERSON', 3), ('?', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\"\"\"Example of training spaCy's named entity recognizer, starting off with an\n",
    "existing model or a blank model.\n",
    "For more details, see the documentation:\n",
    "* Training: https://spacy.io/usage/training\n",
    "* NER: https://spacy.io/usage/linguistic-features#named-entities\n",
    "Compatible with: spaCy v2.0.0+\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "\n",
    "\n",
    "# training data\n",
    "TRAIN_DATA = [\n",
    "    ('Who is Shakira?', {\n",
    "        'entities': [(7, 14, 'PERSON')]\n",
    "    }),\n",
    "    ('I like Europe and Asia.', {\n",
    "        'entities': [(7, 13, 'LOC'), (18, 22, 'LOC')]\n",
    "    })\n",
    "]\n",
    "\n",
    "#testing data\n",
    "TEST_DATA = 'Shakira is a singer. She is popular in both Europe and Asia'\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int))\n",
    "def main(model=None, output_dir=None, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "\n",
    "    # test the trained model\n",
    "    #for text, _ in TEST_DATA:\n",
    "    doc = nlp(text)\n",
    "    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "    print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        #for text, _ in TEST_DATA:\n",
    "        doc = nlp2(text)\n",
    "        print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "            \n",
    "main()\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    plac.call(main)\n",
    "\n",
    "    # Expected output:\n",
    "    # Entities [('Shaka Khan', 'PERSON')]\n",
    "    # Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3),\n",
    "    # ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
    "    # Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
    "    # Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3),\n",
    "# ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
