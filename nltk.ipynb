{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal word tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_text = \"Hello, how are you?\"\n",
    "for i in word_tokenize(example_text):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regexp tokenizer - remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "m\n",
      "interested\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "example_text = \"I'm interested, thanks!\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for i in tokenizer.tokenize(example_text):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer - remove punctuation but keep contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'interested', 'thanks', 'But', 'I', 'ca', \"n't\"]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_text = \"I'm interested, thanks! But I can't.\"\n",
    "tokenized = word_tokenize(example_text)\n",
    "tokenized_contractions = [w for w in tokenized if not re.fullmatch('[' + string.punctuation + ']+', w)]\n",
    "print(tokenized_contractions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_text = \"Hello, how are you? This is an example.\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "words = word_tokenize(example_text)\n",
    "\n",
    "# filtered_sentence = []\n",
    "# for w in words:\n",
    "#     if w not in stop_words:\n",
    "#         filtered_sentence.append(w)\n",
    "\n",
    "filtered_sentence = [w for w in words if not w in stop_words]\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hide\n",
      "in\n",
      "the\n",
      "import\n",
      "queen\n",
      "'s\n",
      "garden\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "example_text = \"Hiding in the important queen's gardens\"\n",
    "words = word_tokenize(example_text)\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Emma', 'NNP'), ('Woodhouse', 'NNP'), (',', ','), ('handsome', 'NN'), (',', ','), ('clever', 'NN'), (',', ','), ('and', 'CC'), ('rich', 'JJ'), (',', ','), ('with', 'IN'), ('a', 'DT'), ('comfortable', 'JJ'), ('home', 'NN'), ('and', 'CC'), ('happy', 'JJ'), ('disposition', 'NN'), (',', ','), ('seemed', 'VBD'), ('to', 'TO'), ('unite', 'VB'), ('some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('blessings', 'NNS'), ('of', 'IN'), ('existence', 'NN'), (';', ':'), ('and', 'CC'), ('had', 'VBD'), ('lived', 'VBN'), ('nearly', 'RB'), ('twenty-one', 'CD'), ('years', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('world', 'NN'), ('with', 'IN'), ('very', 'RB'), ('little', 'JJ'), ('to', 'TO'), ('distress', 'VB'), ('or', 'CC'), ('vex', 'VB'), ('her', 'PRP'), ('.', '.')]\n",
      "[('She', 'PRP'), ('was', 'VBD'), ('the', 'DT'), ('youngest', 'JJS'), ('of', 'IN'), ('the', 'DT'), ('two', 'CD'), ('daughters', 'NNS'), ('of', 'IN'), ('a', 'DT'), ('most', 'RBS'), ('affectionate', 'JJ'), (',', ','), ('indulgent', 'JJ'), ('father', 'NN'), (';', ':'), ('and', 'CC'), ('had', 'VBD'), (',', ','), ('in', 'IN'), ('consequence', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('sister', 'NN'), (\"'s\", 'POS'), ('marriage', 'NN'), (',', ','), ('been', 'VBN'), ('mistress', 'NN'), ('of', 'IN'), ('his', 'PRP$'), ('house', 'NN'), ('from', 'IN'), ('a', 'DT'), ('very', 'RB'), ('early', 'JJ'), ('period', 'NN'), ('.', '.')]\n",
      "[('Her', 'PRP$'), ('mother', 'NN'), ('had', 'VBD'), ('died', 'VBN'), ('too', 'RB'), ('long', 'RB'), ('ago', 'RB'), ('for', 'IN'), ('her', 'PRP$'), ('to', 'TO'), ('have', 'VB'), ('more', 'JJR'), ('than', 'IN'), ('an', 'DT'), ('indistinct', 'JJ'), ('remembrance', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('caresses', 'NNS'), (';', ':'), ('and', 'CC'), ('her', 'PRP$'), ('place', 'NN'), ('had', 'VBD'), ('been', 'VBN'), ('supplied', 'VBN'), ('by', 'IN'), ('an', 'DT'), ('excellent', 'JJ'), ('woman', 'NN'), ('as', 'IN'), ('governess', 'NN'), (',', ','), ('who', 'WP'), ('had', 'VBD'), ('fallen', 'VBN'), ('little', 'JJ'), ('short', 'JJ'), ('of', 'IN'), ('a', 'DT'), ('mother', 'NN'), ('in', 'IN'), ('affection', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize\n",
    "import nltk\n",
    "\n",
    "sample_text = gutenberg.raw(\"sample_text.txt\")\n",
    "pst = PunktSentenceTokenizer()\n",
    "tokenized = pst.tokenize(sample_text)\n",
    "\n",
    "def pos_tagger():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "pos_tagger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize, remove stop words and punctuation, stem and pos tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('emma', 'RB'), ('woodhous', 'JJ'), ('handsom', 'NN'), ('clever', 'NN'), ('rich', 'JJ'), ('comfort', 'NN'), ('home', 'NN'), ('happi', 'NN'), ('disposit', 'NN'), ('seem', 'VBP'), ('unit', 'NN'), ('best', 'JJS'), ('bless', 'JJ'), ('exist', 'VBP'), ('live', 'JJ'), ('nearli', 'NN'), ('twenty-on', 'JJ'), ('year', 'NN'), ('world', 'NN'), ('littl', 'NN'), ('distress', 'NN'), ('vex', 'NN')]\n",
      "[('she', 'PRP'), ('youngest', 'JJS'), ('two', 'CD'), ('daughter', 'NN'), ('affection', 'NN'), ('indulg', 'NN'), ('father', 'NN'), ('consequ', 'NN'), ('sister', 'NN'), (\"'s\", 'POS'), ('marriag', 'NN'), ('mistress', 'NN'), ('hous', 'JJ'), ('earli', 'JJ'), ('period', 'NN')]\n",
      "[('her', 'PRP$'), ('mother', 'NN'), ('die', 'NN'), ('long', 'RB'), ('ago', 'RB'), ('indistinct', 'JJ'), ('remembr', 'NN'), ('caress', 'NN'), ('place', 'NN'), ('suppli', 'JJ'), ('excel', 'NN'), ('woman', 'NN'), ('gover', 'NN'), ('fallen', 'VBN'), ('littl', 'JJ'), ('short', 'JJ'), ('mother', 'NN'), ('affect', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "sample_text = gutenberg.raw(\"sample_text.txt\")\n",
    "pst = PunktSentenceTokenizer()\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokenized = pst.tokenize(sample_text)\n",
    "\n",
    "def tagger():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words_tokenized = word_tokenize(i)\n",
    "            #print(words_tokenized)\n",
    "            words = [w for w in words_tokenized if not re.fullmatch('[' + string.punctuation + ']+', w)]\n",
    "            #print(words)\n",
    "            stopped_words = [w for w in words if not w in stop_words]\n",
    "            #print(stopped_words)\n",
    "            stemmed_words = [ps.stem(w)for w in stopped_words]\n",
    "            #print(stemmed_words)\n",
    "            tagged = nltk.pos_tag(stemmed_words)\n",
    "            print(tagged)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "tagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
