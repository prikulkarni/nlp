{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spacy_ner_colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/prikulkarni/nlp/blob/master/spacy_ner_colab.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "1ogpUueIoJ6Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Install spacy"
      ]
    },
    {
      "metadata": {
        "id": "s7txOladlhW8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "5d9dc845-792f-45ca-98bd-b809fcbe0afd"
      },
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.0.12)\r\n",
            "Requirement already satisfied, skipping upgrade: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from spacy) (2017.4.5)\r\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<0.29,>=0.28 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.28.0)\n",
            "Requirement already satisfied, skipping upgrade: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.8.2)\n",
            "Requirement already satisfied, skipping upgrade: cymem<1.32,>=1.30 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.31.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.14.5)\n",
            "Requirement already satisfied, skipping upgrade: thinc<6.11.0,>=6.10.3 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.10.3)\n",
            "Requirement already satisfied, skipping upgrade: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Requirement already satisfied, skipping upgrade: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.8.13)\n",
            "Requirement already satisfied, skipping upgrade: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.9.0.1)\n",
            "Requirement already satisfied, skipping upgrade: msgpack-numpy<1.0.0,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.4.3.1)\n",
            "Requirement already satisfied, skipping upgrade: msgpack<1.0.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.5.6)\n",
            "Requirement already satisfied, skipping upgrade: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.10.11)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (4.25.0)\n",
            "Requirement already satisfied, skipping upgrade: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy) (0.9.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KRDE2abhrQ6R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Get access to google drive"
      ]
    },
    {
      "metadata": {
        "id": "K16p9HXuzaVd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2366
        },
        "outputId": "c96fc5a1-3e6f-4879-f150-caa145fd4ee7"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package cron.\n",
            "(Reading database ... 18408 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cron_3.0pl1-128ubuntu5_amd64.deb ...\n",
            "Unpacking cron (3.0pl1-128ubuntu5) ...\n",
            "Selecting previously unselected package libapparmor1:amd64.\n",
            "Preparing to unpack .../01-libapparmor1_2.11.0-2ubuntu17.1_amd64.deb ...\n",
            "Unpacking libapparmor1:amd64 (2.11.0-2ubuntu17.1) ...\n",
            "Selecting previously unselected package libdbus-1-3:amd64.\n",
            "Preparing to unpack .../02-libdbus-1-3_1.10.22-1ubuntu1_amd64.deb ...\n",
            "Unpacking libdbus-1-3:amd64 (1.10.22-1ubuntu1) ...\n",
            "Selecting previously unselected package dbus.\n",
            "Preparing to unpack .../03-dbus_1.10.22-1ubuntu1_amd64.deb ...\n",
            "Unpacking dbus (1.10.22-1ubuntu1) ...\n",
            "Selecting previously unselected package dirmngr.\n",
            "Preparing to unpack .../04-dirmngr_2.1.15-1ubuntu8.1_amd64.deb ...\n",
            "Unpacking dirmngr (2.1.15-1ubuntu8.1) ...\n",
            "Selecting previously unselected package distro-info-data.\n",
            "Preparing to unpack .../05-distro-info-data_0.36ubuntu0.2_all.deb ...\n",
            "Unpacking distro-info-data (0.36ubuntu0.2) ...\n",
            "Selecting previously unselected package libkmod2:amd64.\n",
            "Preparing to unpack .../06-libkmod2_24-1ubuntu2_amd64.deb ...\n",
            "Unpacking libkmod2:amd64 (24-1ubuntu2) ...\n",
            "Selecting previously unselected package kmod.\n",
            "Preparing to unpack .../07-kmod_24-1ubuntu2_amd64.deb ...\n",
            "Unpacking kmod (24-1ubuntu2) ...\n",
            "Selecting previously unselected package lsb-release.\n",
            "Preparing to unpack .../08-lsb-release_9.20160110ubuntu5_all.deb ...\n",
            "Unpacking lsb-release (9.20160110ubuntu5) ...\n",
            "Selecting previously unselected package libgirepository-1.0-1:amd64.\n",
            "Preparing to unpack .../09-libgirepository-1.0-1_1.54.1-1_amd64.deb ...\n",
            "Unpacking libgirepository-1.0-1:amd64 (1.54.1-1) ...\n",
            "Selecting previously unselected package gir1.2-glib-2.0:amd64.\n",
            "Preparing to unpack .../10-gir1.2-glib-2.0_1.54.1-1_amd64.deb ...\n",
            "Unpacking gir1.2-glib-2.0:amd64 (1.54.1-1) ...\n",
            "Selecting previously unselected package iso-codes.\n",
            "Preparing to unpack .../11-iso-codes_3.75-1_all.deb ...\n",
            "Unpacking iso-codes (3.75-1) ...\n",
            "Selecting previously unselected package libdbus-glib-1-2:amd64.\n",
            "Preparing to unpack .../12-libdbus-glib-1-2_0.108-2_amd64.deb ...\n",
            "Unpacking libdbus-glib-1-2:amd64 (0.108-2) ...\n",
            "Selecting previously unselected package python-apt-common.\n",
            "Preparing to unpack .../13-python-apt-common_1.4.0~beta3build2_all.deb ...\n",
            "Unpacking python-apt-common (1.4.0~beta3build2) ...\n",
            "Selecting previously unselected package python3-apt.\n",
            "Preparing to unpack .../14-python3-apt_1.4.0~beta3build2_amd64.deb ...\n",
            "Unpacking python3-apt (1.4.0~beta3build2) ...\n",
            "Selecting previously unselected package python3-dbus.\n",
            "Preparing to unpack .../15-python3-dbus_1.2.4-1build3_amd64.deb ...\n",
            "Unpacking python3-dbus (1.2.4-1build3) ...\n",
            "Selecting previously unselected package python3-gi.\n",
            "Preparing to unpack .../16-python3-gi_3.24.1-2build1_amd64.deb ...\n",
            "Unpacking python3-gi (3.24.1-2build1) ...\n",
            "Selecting previously unselected package module-init-tools.\n",
            "Preparing to unpack .../17-module-init-tools_24-1ubuntu2_all.deb ...\n",
            "Unpacking module-init-tools (24-1ubuntu2) ...\n",
            "Selecting previously unselected package python-apt.\n",
            "Preparing to unpack .../18-python-apt_1.4.0~beta3build2_amd64.deb ...\n",
            "Unpacking python-apt (1.4.0~beta3build2) ...\n",
            "Selecting previously unselected package python-pycurl.\n",
            "Preparing to unpack .../19-python-pycurl_7.43.0-2build2_amd64.deb ...\n",
            "Unpacking python-pycurl (7.43.0-2build2) ...\n",
            "Selecting previously unselected package python-software-properties.\n",
            "Preparing to unpack .../20-python-software-properties_0.96.24.17_all.deb ...\n",
            "Unpacking python-software-properties (0.96.24.17) ...\n",
            "Selecting previously unselected package python3-software-properties.\n",
            "Preparing to unpack .../21-python3-software-properties_0.96.24.17_all.deb ...\n",
            "Unpacking python3-software-properties (0.96.24.17) ...\n",
            "Selecting previously unselected package software-properties-common.\n",
            "Preparing to unpack .../22-software-properties-common_0.96.24.17_all.deb ...\n",
            "Unpacking software-properties-common (0.96.24.17) ...\n",
            "Selecting previously unselected package unattended-upgrades.\n",
            "Preparing to unpack .../23-unattended-upgrades_0.98ubuntu1.1_all.deb ...\n",
            "Unpacking unattended-upgrades (0.98ubuntu1.1) ...\n",
            "Setting up python-apt-common (1.4.0~beta3build2) ...\n",
            "Setting up python3-apt (1.4.0~beta3build2) ...\n",
            "Setting up iso-codes (3.75-1) ...\n",
            "Setting up distro-info-data (0.36ubuntu0.2) ...\n",
            "Setting up python-pycurl (7.43.0-2build2) ...\n",
            "Setting up lsb-release (9.20160110ubuntu5) ...\n",
            "Setting up libgirepository-1.0-1:amd64 (1.54.1-1) ...\n",
            "Setting up libkmod2:amd64 (24-1ubuntu2) ...\n",
            "Setting up gir1.2-glib-2.0:amd64 (1.54.1-1) ...\n",
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "Setting up libapparmor1:amd64 (2.11.0-2ubuntu17.1) ...\n",
            "Setting up unattended-upgrades (0.98ubuntu1.1) ...\n",
            "\n",
            "Creating config file /etc/apt/apt.conf.d/20auto-upgrades with new version\n",
            "\n",
            "Creating config file /etc/apt/apt.conf.d/50unattended-upgrades with new version\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up dirmngr (2.1.15-1ubuntu8.1) ...\n",
            "Setting up cron (3.0pl1-128ubuntu5) ...\n",
            "Adding group `crontab' (GID 102) ...\n",
            "Done.\n",
            "update-rc.d: warning: start and stop actions are no longer supported; falling back to defaults\n",
            "update-rc.d: warning: stop runlevel arguments (1) do not match cron Default-Stop values (none)\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libdbus-1-3:amd64 (1.10.22-1ubuntu1) ...\n",
            "Setting up kmod (24-1ubuntu2) ...\n",
            "Setting up libdbus-glib-1-2:amd64 (0.108-2) ...\n",
            "Setting up python3-gi (3.24.1-2build1) ...\n",
            "Setting up module-init-tools (24-1ubuntu2) ...\n",
            "Setting up python3-software-properties (0.96.24.17) ...\n",
            "Setting up dbus (1.10.22-1ubuntu1) ...\n",
            "Setting up python-apt (1.4.0~beta3build2) ...\n",
            "Setting up python3-dbus (1.2.4-1build3) ...\n",
            "Setting up python-software-properties (0.96.24.17) ...\n",
            "Setting up software-properties-common (0.96.24.17) ...\n",
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "Processing triggers for dbus (1.10.22-1ubuntu1) ...\n",
            "gpg: keybox '/tmp/tmpcmmalat0/pubring.gpg' created\n",
            "gpg: /tmp/tmpcmmalat0/trustdb.gpg: trustdb created\n",
            "gpg: key AD5F235DF639B041: public key \"Launchpad PPA for Alessandro Strada\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
            "Selecting previously unselected package libfuse2:amd64.\n",
            "(Reading database ... 19816 files and directories currently installed.)\n",
            "Preparing to unpack .../libfuse2_2.9.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking libfuse2:amd64 (2.9.7-1ubuntu1) ...\n",
            "Selecting previously unselected package fuse.\n",
            "Preparing to unpack .../fuse_2.9.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking fuse (2.9.7-1ubuntu1) ...\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.6.21-0ubuntu2_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.6.21-0ubuntu2) ...\n",
            "Setting up libfuse2:amd64 (2.9.7-1ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "Setting up fuse (2.9.7-1ubuntu1) ...\n",
            "Setting up google-drive-ocamlfuse (0.6.21-0ubuntu2) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "octstzzfzeYD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LdV1SGXmpIY6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9b91f73-89fc-4ffb-c3d9-95a3a532ecb6"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  drive  sample_data\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Dehfegyy1EcA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d366f44-bf4b-47b2-b528-3341e1743026"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "ZLyT8aYb1OPo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "8f14220d-d8a4-4673-8ebe-f60a55724b79"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 6975725286382621616, name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 11281989632\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 16304593802951983396\n",
              " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "qbpO7XX-7Bw_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf316672-a36b-4b50-f4e5-164d01cb6f6a"
      },
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.3\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DZSm61IV3DNX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Customised NER"
      ]
    },
    {
      "metadata": {
        "id": "2rhxJM9q3LUu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "4fee1639-3ae7-45be-fc32-039d2b3186c4"
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "import plac\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "def offseter(lbl, doc, matchitem):\n",
        "    o_one = len(str(doc[0:matchitem[1]]))\n",
        "    subdoc = doc[matchitem[1]:matchitem[2]]\n",
        "    o_two = o_one + len(str(subdoc))\n",
        "    return (o_one, o_two, lbl)\n",
        "\n",
        "#nlp = spacy.blank('en')\n",
        "nlp = spacy.load('en')\n",
        "#output_dir = \"drive/models/angela\"\n",
        "#nlp = spacy.load(output_dir)\n",
        "#print(\"Loading from\", output_dir)\n",
        "\n",
        "if 'ner' not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe('ner')\n",
        "    nlp.add_pipe(ner)\n",
        "else:\n",
        "    ner = nlp.get_pipe('ner')\n",
        "\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "\n",
        "#ner.add_label(label)\n",
        "\n",
        "label = 'CHANCEL'\n",
        "for i in ['Angela Merkel', 'Angela', 'Merkel',]:\n",
        "    matcher.add(label, None, nlp(i))\n",
        "\n",
        "\n",
        "# label1 = 'PRESID'\n",
        "# for i in ['Vladimir Putin', 'Vladimir', 'Putin',]:\n",
        "#     matcher.add(label1, None, nlp(i))\n",
        "\n",
        "ner.add_label(label)\n",
        "#ner.add_label(label1)\n",
        "\n",
        "res = []\n",
        "to_train_ents = []\n",
        "with open('drive/data/angela_merkel.txt') as am:\n",
        "    line = True\n",
        "    while line:\n",
        "        line = am.readline()\n",
        "        mnlp_line = nlp(line)\n",
        "        matches = matcher(mnlp_line)\n",
        "        res = [offseter(label, mnlp_line, x)\n",
        "               for x\n",
        "               in matches]\n",
        "        to_train_ents.append((line, dict(entities=res)))\n",
        "#         res1 = [offseter(label1, mnlp_line, x)\n",
        "#                for x\n",
        "#                in matches]\n",
        "#         to_train_ents.append((line, dict(entities=res1)))\n",
        "        \n",
        "@plac.annotations(\n",
        "    new_model_name=(\"New model name for model meta.\", \"option\", \"nm\", str),\n",
        "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path))\n",
        "def train(new_model_name='angela', output_dir=None):\n",
        "  \n",
        "    optimizer = nlp.begin_training()\n",
        "    \n",
        "    other_pipes = [pipe\n",
        "                   for pipe\n",
        "                   in nlp.pipe_names\n",
        "                   if pipe != 'ner']\n",
        "    \n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        for itn in range(20):\n",
        "            losses = {}\n",
        "            random.shuffle(to_train_ents)\n",
        "            for item in to_train_ents:\n",
        "                nlp.update([item[0]],\n",
        "                           [item[1]],\n",
        "                           sgd=optimizer,\n",
        "                           drop=0.35,\n",
        "                           losses=losses)\n",
        "            print(losses)\n",
        "\n",
        "    if output_dir is None:\n",
        "        output_dir = \"drive/models/angela\"\n",
        "        \n",
        "    noutput_dir = Path(output_dir)\n",
        "    if not noutput_dir.exists():\n",
        "        noutput_dir.mkdir()\n",
        "        \n",
        "    nlp.meta['name'] = new_model_name\n",
        "    nlp.to_disk(output_dir)\n",
        "        \n",
        "    random.shuffle(to_train_ents)\n",
        "\n",
        "    test_text = to_train_ents[0][0]\n",
        "    #test_text = \"Merkel had a meeting with Putin.\"\n",
        "    doc = nlp(test_text)\n",
        "    print(\"Entities in '%s'\" % test_text)\n",
        "    for ent in doc.ents:\n",
        "        print(ent.label_, ent.text)\n",
        "\n",
        "train()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
            "{'ner': 173.74445107543806}\n",
            "{'ner': 102.9416562527794}\n",
            "{'ner': 93.00138173953404}\n",
            "{'ner': 92.33353682925804}\n",
            "{'ner': 99.63629841558858}\n",
            "{'ner': 101.31497825984094}\n",
            "{'ner': 98.86415659651185}\n",
            "{'ner': 105.24894067756175}\n",
            "{'ner': 93.71357046286384}\n",
            "{'ner': 92.93112548411149}\n",
            "{'ner': 80.39163961015095}\n",
            "{'ner': 87.70076541640069}\n",
            "{'ner': 99.08991635996188}\n",
            "{'ner': 106.26035018736168}\n",
            "{'ner': 102.69900018218374}\n",
            "{'ner': 79.65151813520843}\n",
            "{'ner': 93.38510100399553}\n",
            "{'ner': 98.30610358305337}\n",
            "{'ner': 97.47421664615672}\n",
            "{'ner': 97.6742594244663}\n",
            "Entities in 'Chancellor Angela Merkel said Saturday she expected \"no special\" outcomes from her largely private \"working meeting\" with Putin at Meseberg Palace, a German state guest house an hour's drive north of Berlin.\n",
            "'\n",
            "CHANCEL Merkel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lWDVHIsjK6Y8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "ceac4356-5ee5-4b07-b8dc-deea59a88a86"
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "import plac\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "def offseter(lbl, doc, matchitem):\n",
        "    o_one = len(str(doc[0:matchitem[1]]))\n",
        "    subdoc = doc[matchitem[1]:matchitem[2]]\n",
        "    o_two = o_one + len(str(subdoc))\n",
        "    return (o_one, o_two, lbl)\n",
        "\n",
        "output_dir = \"drive/models/angela\"\n",
        "nlp = spacy.load(output_dir)\n",
        "print(\"Loading from\", output_dir)\n",
        "\n",
        "if 'ner' not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe('ner')\n",
        "    nlp.add_pipe(ner)\n",
        "else:\n",
        "    ner = nlp.get_pipe('ner')\n",
        "\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "\n",
        "# label1 = 'PRESID'\n",
        "# for i in ['Vladimir Putin', 'Vladimir', 'Putin',]:\n",
        "#     matcher.add(label1, None, nlp(i))\n",
        "\n",
        "# ner.add_label(label1)\n",
        "\n",
        "res = []\n",
        "to_train_ents = []\n",
        "with open('drive/data/angela_merkel.txt') as am:\n",
        "    line = True\n",
        "    while line:\n",
        "        line = am.readline()\n",
        "        mnlp_line = nlp(line)\n",
        "        matches = matcher(mnlp_line)\n",
        "        res = [offseter(label, mnlp_line, x)\n",
        "               for x\n",
        "               in matches]\n",
        "        to_train_ents.append((line, dict(entities=res)))\n",
        "        res1 = [offseter(label1, mnlp_line, x)\n",
        "               for x\n",
        "               in matches]\n",
        "        to_train_ents.append((line, dict(entities=res1)))\n",
        "        \n",
        "# @plac.annotations(\n",
        "#     new_model_name=(\"New model name for model meta.\", \"option\", \"nm\", str),\n",
        "#     output_dir=(\"Optional output directory\", \"option\", \"o\", Path))\n",
        "def train():\n",
        "  \n",
        "    optimizer = nlp.begin_training()\n",
        "    \n",
        "    other_pipes = [pipe\n",
        "                   for pipe\n",
        "                   in nlp.pipe_names\n",
        "                   if pipe != 'ner']\n",
        "    \n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        for itn in range(20):\n",
        "            losses = {}\n",
        "            random.shuffle(to_train_ents)\n",
        "            for item in to_train_ents:\n",
        "                nlp.update([item[0]],\n",
        "                           [item[1]],\n",
        "                           sgd=optimizer,\n",
        "                           drop=0.35,\n",
        "                           losses=losses)\n",
        "            print(losses)\n",
        "\n",
        "    output_dir = \"drive/models/angela\"\n",
        "        \n",
        "#     noutput_dir = Path(output_dir)\n",
        "#     if not noutput_dir.exists():\n",
        "#         noutput_dir.mkdir()\n",
        "        \n",
        "    #nlp.meta['name'] = new_model_name\n",
        "    nlp.to_disk(output_dir)\n",
        "        \n",
        "    random.shuffle(to_train_ents)\n",
        "\n",
        "    test_text = to_train_ents[0][0]\n",
        "    #test_text = \"Merkel had a meeting with Putin.\"\n",
        "    doc = nlp(test_text)\n",
        "    print(\"Entities in '%s'\" % test_text)\n",
        "    for ent in doc.ents:\n",
        "        print(ent.label_, ent.text)\n",
        "\n",
        "train()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading from drive/models/angela\n",
            "{'ner': 3.4321834452984623}\n",
            "{'ner': 1.7412152510469334e-10}\n",
            "{'ner': 9.065124844037724e-13}\n",
            "{'ner': 2.514983265468342e-12}\n",
            "{'ner': 4.412116847037798e-12}\n",
            "{'ner': 7.144525218460551e-10}\n",
            "{'ner': 2.662367437908956e-12}\n",
            "{'ner': 7.563513025217692e-09}\n",
            "{'ner': 5.762896914477426e-15}\n",
            "{'ner': 1.6698920223583376e-11}\n",
            "{'ner': 5.895119745754372e-12}\n",
            "{'ner': 9.33615474315164e-12}\n",
            "{'ner': 3.609067579151486e-11}\n",
            "{'ner': 1.9425484491820584e-13}\n",
            "{'ner': 2.0662835726337913e-10}\n",
            "{'ner': 1.0116392631618534e-12}\n",
            "{'ner': 3.170202931342149e-09}\n",
            "{'ner': 2.0620950342310205e-14}\n",
            "{'ner': 4.0795843403088606e-13}\n",
            "{'ner': 1.305186358867908e-13}\n",
            "Entities in '\n",
            "'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wdO46D_SZEvB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "6256330a-386f-472e-9940-ae77a675c902"
      },
      "cell_type": "code",
      "source": [
        "import spacy \n",
        "#from spacy.lang.en import English\n",
        "#nlp = English().from_disk('drive/models/angela')\n",
        "output_dir = \"drive/models/angela\"\n",
        "print(\"Loading from\", output_dir)\n",
        "nlp = spacy.load(output_dir)\n",
        "#nlp.entity.cfg['extra_labels']\n",
        "print(nlp.entity.move_names)\n",
        "test_text = \"German Chancellor Angela Merkel has a meeting with Russian President Putin\"\n",
        "doc = nlp(test_text)\n",
        "print(\"Entities in '%s'\" % test_text)\n",
        "for ent in doc.ents:\n",
        "  print(ent.label_, ent.text)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading from drive/models/angela\n",
            "['B-PERSON', 'B-CARDINAL', 'B-ORG', 'B-GPE', 'B-FAC', 'B-MONEY', 'B-NORP', 'B-DATE', 'B-TIME', 'B-ORDINAL', 'B-PERCENT', 'B-PRODUCT', 'B-LANGUAGE', 'B-LOC', 'B-QUANTITY', 'B-WORK_OF_ART', 'B-EVENT', 'B-LAW', 'I-PERSON', 'I-CARDINAL', 'I-ORG', 'I-GPE', 'I-FAC', 'I-MONEY', 'I-NORP', 'I-DATE', 'I-TIME', 'I-ORDINAL', 'I-PERCENT', 'I-PRODUCT', 'I-LANGUAGE', 'I-LOC', 'I-QUANTITY', 'I-WORK_OF_ART', 'I-EVENT', 'I-LAW', 'L-PERSON', 'L-CARDINAL', 'L-ORG', 'L-GPE', 'L-FAC', 'L-MONEY', 'L-NORP', 'L-DATE', 'L-TIME', 'L-ORDINAL', 'L-PERCENT', 'L-PRODUCT', 'L-LANGUAGE', 'L-LOC', 'L-QUANTITY', 'L-WORK_OF_ART', 'L-EVENT', 'L-LAW', 'U-PERSON', 'U-CARDINAL', 'U-ORG', 'U-GPE', 'U-FAC', 'U-MONEY', 'U-NORP', 'U-DATE', 'U-TIME', 'U-ORDINAL', 'U-PERCENT', 'U-PRODUCT', 'U-LANGUAGE', 'U-LOC', 'U-QUANTITY', 'U-WORK_OF_ART', 'U-EVENT', 'U-LAW', 'O', 'B-CHANCEL', 'I-CHANCEL', 'L-CHANCEL', 'U-CHANCEL', 'B-PRESID', 'I-PRESID', 'L-PRESID', 'U-PRESID']\n",
            "Entities in 'German Chancellor Angela Merkel has a meeting with Russian President Putin'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "adKCgBse7tnJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Spacy Documentation code"
      ]
    },
    {
      "metadata": {
        "id": "y1AU2V2R72Ai",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1822
        },
        "outputId": "227b0484-90d2-4b7b-984c-6f3829ebf4db"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function\n",
        "\n",
        "import plac\n",
        "import random\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "\n",
        "\n",
        "# training data\n",
        "TRAIN_DATA = [\n",
        "    ('Who is Shakira?', {\n",
        "        'entities': [(7, 14, 'PERSON')]\n",
        "    }),\n",
        "    ('I like Europe and Asia.', {\n",
        "        'entities': [(7, 13, 'LOC'), (18, 22, 'LOC')]\n",
        "    })\n",
        "]\n",
        "\n",
        "#testing data\n",
        "TEST_DATA = 'Shakira is a singer. She is popular in both Europe and Asia'\n",
        "\n",
        "\n",
        "@plac.annotations(\n",
        "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
        "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
        "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int))\n",
        "def main(model=None, output_dir=None, n_iter=100):\n",
        "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
        "    if model is not None:\n",
        "        nlp = spacy.load(model)  # load existing spaCy model\n",
        "        print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "        nlp = spacy.blank('en')  # create blank Language class\n",
        "        print(\"Created blank 'en' model\")\n",
        "        \n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if 'ner' not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe('ner')\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "    # otherwise, get it so we can add labels\n",
        "    else:\n",
        "        ner = nlp.get_pipe('ner')\n",
        "\n",
        "    # add labels\n",
        "    for _, annotations in TRAIN_DATA:\n",
        "        for ent in annotations.get('entities'):\n",
        "            ner.add_label(ent[2])\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        optimizer = nlp.begin_training()\n",
        "        for itn in range(n_iter):\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            losses = {}\n",
        "            for text, annotations in TRAIN_DATA:\n",
        "                nlp.update(\n",
        "                    [text],  # batch of texts\n",
        "                    [annotations],  # batch of annotations\n",
        "                    drop=0.5,  # dropout - make it harder to memorise data\n",
        "                    sgd=optimizer,  # callable to update weights\n",
        "                    losses=losses)\n",
        "            print(losses)\n",
        "            \n",
        "    # test the trained model\n",
        "    #for text, _ in TEST_DATA:\n",
        "    text = TEST_DATA\n",
        "    doc = nlp(text)\n",
        "    print(nlp.entity.move_names)\n",
        "    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
        "    print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
        "\n",
        "    # save model to output directory\n",
        "    if output_dir is not None:\n",
        "        output_dir = Path(output_dir)\n",
        "        if not output_dir.exists():\n",
        "            output_dir.mkdir()\n",
        "        nlp.to_disk(output_dir)\n",
        "        print(\"Saved model to\", output_dir)\n",
        "\n",
        "        # test the saved model\n",
        "        print(\"Loading from\", output_dir)\n",
        "        nlp2 = spacy.load(output_dir)\n",
        "        #for text, _ in TEST_DATA:\n",
        "        doc = nlp2(text)\n",
        "        print(nlp2.entity.move_names)\n",
        "        print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
        "        print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
        "\n",
        "            \n",
        "main()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created blank 'en' model\n",
            "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
            "{'ner': 8.464841950684786}\n",
            "{'ner': 9.539180871332064}\n",
            "{'ner': 14.378742635250092}\n",
            "{'ner': 8.862236209874027}\n",
            "{'ner': 8.156067713886658}\n",
            "{'ner': 8.783401463919915}\n",
            "{'ner': 8.757857236324076}\n",
            "{'ner': 9.635891117231788}\n",
            "{'ner': 3.963297861298565}\n",
            "{'ner': 5.525518167817382}\n",
            "{'ner': 4.989305564356019}\n",
            "{'ner': 10.916562724253527}\n",
            "{'ner': 7.2199440604531615}\n",
            "{'ner': 0.04891543866688441}\n",
            "{'ner': 0.0004499736704655606}\n",
            "{'ner': 2.0074987859583078}\n",
            "{'ner': 2.000294718656887}\n",
            "{'ner': 1.4683032412422963e-05}\n",
            "{'ner': 1.9680268730233648}\n",
            "{'ner': 1.0932478362961655}\n",
            "{'ner': 3.7398193811467184}\n",
            "{'ner': 3.064042616382443}\n",
            "{'ner': 1.9851628033032405}\n",
            "{'ner': 0.31719812750816384}\n",
            "{'ner': 0.19036422763488567}\n",
            "{'ner': 1.988214451646076}\n",
            "{'ner': 4.1466865657052136e-05}\n",
            "{'ner': 0.012779633874253803}\n",
            "{'ner': 2.4967281278496412e-05}\n",
            "{'ner': 7.154414071209226e-06}\n",
            "{'ner': 1.3157078476760944}\n",
            "{'ner': 3.9352342894508817e-13}\n",
            "{'ner': 0.02238365009328181}\n",
            "{'ner': 7.624213809763367e-06}\n",
            "{'ner': 7.734143012247909e-09}\n",
            "{'ner': 2.452568887861261e-05}\n",
            "{'ner': 4.4090009427434563e-10}\n",
            "{'ner': 1.4921740147334324e-10}\n",
            "{'ner': 0.13110733032226562}\n",
            "{'ner': 6.450249975039837e-09}\n",
            "{'ner': 0.03062905184939255}\n",
            "{'ner': 1.080767843407675e-21}\n",
            "{'ner': 5.6674151195102306e-12}\n",
            "{'ner': 8.587010228118737e-18}\n",
            "{'ner': 7.961664688382285e-14}\n",
            "{'ner': 5.412647333742194e-19}\n",
            "{'ner': 1.0301145457751943e-14}\n",
            "{'ner': 1.1481463333858337e-12}\n",
            "{'ner': 2.2268889821482e-11}\n",
            "{'ner': 3.8339021478230256e-18}\n",
            "{'ner': 3.7146084819195124e-12}\n",
            "{'ner': 5.2611105247076654e-15}\n",
            "{'ner': 1.6599312857603723e-07}\n",
            "{'ner': 1.6406656503677526}\n",
            "{'ner': 1.6121212656024894e-19}\n",
            "{'ner': 1.6350159293470683e-21}\n",
            "{'ner': 2.8893253073258586e-20}\n",
            "{'ner': 1.2741476496124073e-07}\n",
            "{'ner': 8.743754165213843e-08}\n",
            "{'ner': 3.2271079362943816e-17}\n",
            "{'ner': 1.1940795629265882e-13}\n",
            "{'ner': 4.2607213708299556e-14}\n",
            "{'ner': 2.8849123016188034e-17}\n",
            "{'ner': 3.611662435787375e-08}\n",
            "{'ner': 1.3067790448351581e-06}\n",
            "{'ner': 9.518266916214943e-29}\n",
            "{'ner': 3.641677390492222e-16}\n",
            "{'ner': 5.217797956313866e-16}\n",
            "{'ner': 7.959812488193654e-23}\n",
            "{'ner': 1.510306289753733e-11}\n",
            "{'ner': 1.5244786336299919e-15}\n",
            "{'ner': 2.315427077909152e-16}\n",
            "{'ner': 6.959654225608418e-06}\n",
            "{'ner': 3.146488631311506e-07}\n",
            "{'ner': 8.356247277796332e-24}\n",
            "{'ner': 6.671667044765333e-22}\n",
            "{'ner': 2.1708655493968006e-20}\n",
            "{'ner': 6.637821822287471e-20}\n",
            "{'ner': 2.7476861136539242e-14}\n",
            "{'ner': 1.401146768236805e-19}\n",
            "{'ner': 7.143880338847055e-06}\n",
            "{'ner': 2.7219443683797027e-18}\n",
            "{'ner': 4.860865032285209e-14}\n",
            "{'ner': 9.070283825578239e-21}\n",
            "{'ner': 1.954217664698141e-15}\n",
            "{'ner': 1.439157965340874e-24}\n",
            "{'ner': 4.851846226539393e-15}\n",
            "{'ner': 9.570968478856555e-19}\n",
            "{'ner': 5.791459534396532e-10}\n",
            "{'ner': 4.776055106958672e-07}\n",
            "{'ner': 3.0198805123724932e-21}\n",
            "{'ner': 7.319005785392423e-24}\n",
            "{'ner': 2.6639339746979072e-16}\n",
            "{'ner': 4.46274217580367e-09}\n",
            "{'ner': 0.2985671162605468}\n",
            "{'ner': 1.650178407182077e-14}\n",
            "{'ner': 1.1848769788645082e-07}\n",
            "{'ner': 2.480914247111005e-24}\n",
            "{'ner': 7.1118734525472645e-19}\n",
            "{'ner': 5.969284094797999e-11}\n",
            "['B-PERSON', 'I-PERSON', 'L-PERSON', 'U-PERSON', 'B-LOC', 'I-LOC', 'L-LOC', 'U-LOC', 'O']\n",
            "Entities [('Europe', 'LOC'), ('Asia', 'LOC')]\n",
            "Tokens [('Shakira', '', 2), ('is', '', 2), ('a', '', 2), ('singer', '', 2), ('.', '', 2), ('She', '', 2), ('is', '', 2), ('popular', '', 2), ('in', '', 2), ('both', '', 2), ('Europe', 'LOC', 3), ('and', '', 2), ('Asia', 'LOC', 3)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WDVYskhmoDC0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dataturks annotated data - NER"
      ]
    },
    {
      "metadata": {
        "id": "PHhTaoz1hJQc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1261
        },
        "outputId": "05e77b20-8347-4473-9a39-c316a1a2e60f"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from spacy.gold import GoldParse\n",
        "from spacy.scorer import Scorer\n",
        "from sklearn.metrics import accuracy_score\n",
        "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
        "    try:\n",
        "        training_data = []\n",
        "        lines=[]\n",
        "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            data = json.loads(line)\n",
        "            text = data['content']\n",
        "            entities = []\n",
        "            for annotation in data['annotation']:\n",
        "                #only a single point in text annotation.\n",
        "                point = annotation['points'][0]\n",
        "                labels = annotation['label']\n",
        "                # handle both list of labels or a single label.\n",
        "                if not isinstance(labels, list):\n",
        "                    labels = [labels]\n",
        "\n",
        "                for label in labels:\n",
        "                    #dataturks indices are both inclusive [start, end] but spacy is not [start, end)\n",
        "                    entities.append((point['start'], point['end'] + 1 ,label))\n",
        "\n",
        "\n",
        "            training_data.append((text, {\"entities\" : entities}))\n",
        "\n",
        "        return training_data\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
        "        return None\n",
        "\n",
        "\n",
        "import spacy\n",
        "################### Train Spacy NER.###########\n",
        "def train_spacy():\n",
        "\n",
        "    TRAIN_DATA = convert_dataturks_to_spacy(\"drive/data/traindata.json\")\n",
        "    nlp = spacy.blank('en')  # create blank Language class\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if 'ner' not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe('ner')\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "       \n",
        "\n",
        "    # add labels\n",
        "    for _, annotations in TRAIN_DATA:\n",
        "         for ent in annotations.get('entities'):\n",
        "            ner.add_label(ent[2])\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        optimizer = nlp.begin_training()\n",
        "        for itn in range(10):\n",
        "            print(\"Starting iteration \" + str(itn))\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            losses = {}\n",
        "            for text, annotations in TRAIN_DATA:\n",
        "                nlp.update(\n",
        "                    [text],  # batch of texts\n",
        "                    [annotations],  # batch of annotations\n",
        "                    drop=0.2,  # dropout - make it harder to memorise data\n",
        "                    sgd=optimizer,  # callable to update weights\n",
        "                    losses=losses)\n",
        "            print(losses)\n",
        "    #test the model and evaluate it\n",
        "    examples = convert_dataturks_to_spacy(\"drive/data/testdata.json\")\n",
        "    tp=0\n",
        "    tr=0\n",
        "    tf=0\n",
        "\n",
        "    ta=0\n",
        "    c=0        \n",
        "    for text,annot in examples:\n",
        "\n",
        "        f=open(\"resume\"+str(c)+\".txt\",\"w\")\n",
        "        doc_to_test=nlp(text)\n",
        "        d={}\n",
        "        for ent in doc_to_test.ents:\n",
        "            d[ent.label_]=[]\n",
        "        for ent in doc_to_test.ents:\n",
        "            d[ent.label_].append(ent.text)\n",
        "\n",
        "        for i in set(d.keys()):\n",
        "\n",
        "            f.write(\"\\n\\n\")\n",
        "            f.write(i +\":\"+\"\\n\")\n",
        "            for j in set(d[i]):\n",
        "                f.write(j.replace('\\n','')+\"\\n\")\n",
        "        d={}\n",
        "        for ent in doc_to_test.ents:\n",
        "            d[ent.label_]=[0,0,0,0,0,0]\n",
        "        for ent in doc_to_test.ents:\n",
        "            doc_gold_text= nlp.make_doc(text)\n",
        "            gold = GoldParse(doc_gold_text, entities=annot.get(\"entities\"))\n",
        "            y_true = [ent.label_ if ent.label_ in x else 'Not '+ent.label_ for x in gold.ner]\n",
        "            y_pred = [x.ent_type_ if x.ent_type_ ==ent.label_ else 'Not '+ent.label_ for x in doc_to_test]  \n",
        "            if(d[ent.label_][0]==0):\n",
        "                #f.write(\"For Entity \"+ent.label_+\"\\n\")   \n",
        "                #f.write(classification_report(y_true, y_pred)+\"\\n\")\n",
        "                (p,r,f,s)= precision_recall_fscore_support(y_true,y_pred,average='weighted')\n",
        "                a=accuracy_score(y_true,y_pred)\n",
        "                d[ent.label_][0]=1\n",
        "                d[ent.label_][1]+=p\n",
        "                d[ent.label_][2]+=r\n",
        "                d[ent.label_][3]+=f\n",
        "                d[ent.label_][4]+=a\n",
        "                d[ent.label_][5]+=1\n",
        "        c+=1\n",
        "    for i in d:\n",
        "        print(\"\\n For Entity \"+i+\"\\n\")\n",
        "        print(\"Accuracy : \"+str((d[i][4]/d[i][5])*100)+\"%\")\n",
        "        print(\"Precision : \"+str(d[i][1]/d[i][5]))\n",
        "        print(\"Recall : \"+str(d[i][2]/d[i][5]))\n",
        "        print(\"F-score : \"+str(d[i][3]/d[i][5]))\n",
        "train_spacy()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
            "Statring iteration 0\n",
            "{'ner': 1638.9786479917047}\n",
            "Statring iteration 1\n",
            "{'ner': 605.3991683064176}\n",
            "Statring iteration 2\n",
            "{'ner': 535.7001255856526}\n",
            "Statring iteration 3\n",
            "{'ner': 431.86820218492323}\n",
            "Statring iteration 4\n",
            "{'ner': 365.46321266512564}\n",
            "Statring iteration 5\n",
            "{'ner': 259.6570975410603}\n",
            "Statring iteration 6\n",
            "{'ner': 251.85436714328213}\n",
            "Statring iteration 7\n",
            "{'ner': 211.92943803246865}\n",
            "Statring iteration 8\n",
            "{'ner': 226.48755159647297}\n",
            "Statring iteration 9\n",
            "{'ner': 146.140918776508}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " For Entity Name\n",
            "\n",
            "Accuracy : 100.0%\n",
            "Precision : 1.0\n",
            "Recall : 1.0\n",
            "F-score : 1.0\n",
            "\n",
            " For Entity Location\n",
            "\n",
            "Accuracy : 99.46524064171123%\n",
            "Precision : 0.9946810644748985\n",
            "Recall : 0.9946524064171123\n",
            "F-score : 0.9931741475187439\n",
            "\n",
            " For Entity Designation\n",
            "\n",
            "Accuracy : 98.6096256684492%\n",
            "Precision : 0.9862899851551261\n",
            "Recall : 0.986096256684492\n",
            "F-score : 0.9808288914349038\n",
            "\n",
            " For Entity Graduation Year\n",
            "\n",
            "Accuracy : 99.89304812834224%\n",
            "Precision : 1.0\n",
            "Recall : 0.9989304812834224\n",
            "F-score : 0.9994649545211343\n",
            "\n",
            " For Entity College Name\n",
            "\n",
            "Accuracy : 97.54010695187165%\n",
            "Precision : 1.0\n",
            "Recall : 0.9754010695187165\n",
            "F-score : 0.9875473741201949\n",
            "\n",
            " For Entity Companies worked at\n",
            "\n",
            "Accuracy : 99.25133689839572%\n",
            "Precision : 0.9925694786382531\n",
            "Recall : 0.9925133689839573\n",
            "F-score : 0.9896159815431554\n",
            "\n",
            " For Entity Degree\n",
            "\n",
            "Accuracy : 99.46524064171123%\n",
            "Precision : 0.994681033791753\n",
            "Recall : 0.9946524064171123\n",
            "F-score : 0.9927497276175877\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}